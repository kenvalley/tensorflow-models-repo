{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f84b55",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "903a0c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(404)\n",
    "\n",
    "from time import strftime\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97fcf5b",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d1ce41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH = 'MNIST/digit_xtrain.csv'\n",
    "X_TEST_PATH = 'MNIST/digit_xtest.csv'\n",
    "Y_TRAIN_PATH = 'MNIST/digit_ytrain.csv'\n",
    "Y_TEST_PATH = 'MNIST/digit_ytest.csv'\n",
    "\n",
    "LOGGING_PATH = 'tensorboard_mnist_digit_logs/'\n",
    "LOGGING_PATH_2 = 'tensorboard_mnist_digit_logs_new/'\n",
    "\n",
    "\n",
    "NR_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "CHANNELS = 1\n",
    "TOTAL_INPUTS = IMAGE_WIDTH*IMAGE_HEIGHT*CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e1667",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a2461798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 109 ms\n",
      "Wall time: 107 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b09d3761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f79ef17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0c292a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e0260913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 17.9 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c93996d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.89 s\n",
      "Wall time: 2.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=int )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0623398d",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8d3fd584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1e2e9dd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "99b85e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ed96fe2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "87c98fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all[:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c81fc9",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8283d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale features\n",
    "x_train_all, x_test = x_train_all/255.0, x_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155f25f",
   "metadata": {},
   "source": [
    "#### Convert target values to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9ce2c9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = y_train_all[:5]\n",
    "np.eye(10)[values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fb00590d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0a1dce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.eye(NR_CLASSES)[y_train_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "54c65b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "312298b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.eye(NR_CLASSES)[y_test]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77abbe61",
   "metadata": {},
   "source": [
    "## Create Validation dataset from training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694df1f",
   "metadata": {},
   "source": [
    "**Challenge:** Split the training dataset into smaller training datatset and a validation dataset for the features and the labels. Create four arrays: x_val, y_val, x_train and y_train from x_train_all and y_train_all. Use validation size of 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4dbd4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train_all[:VALIDATION_SIZE]\n",
    "y_val = y_train_all[:VALIDATION_SIZE]\n",
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "297b92a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 10), (10000, 10))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d7653",
   "metadata": {},
   "source": [
    "## Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "19c781f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=[None, TOTAL_INPUTS], name='X')\n",
    "Y = tf.compat.v1.placeholder(tf.float32, shape=[None, NR_CLASSES], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d0546",
   "metadata": {},
   "source": [
    "### Neural Network Architecture\n",
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dffcc81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Hidden layers\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c1ccab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.compat.v1.name_scope('First_layer'):\n",
    "\n",
    "\n",
    "#     initial_w1 = tf.compat.v1.truncated_normal(shape=[TOTAL_INPUTS, n_hidden1], stddev=0.1, seed=42)\n",
    "#     w1 = tf.compat.v1.Variable(initial_value=initial_w1, name='w1')\n",
    "\n",
    "#     initial_b1 = tf.constant(value=0.0, shape=[n_hidden1])\n",
    "#     b1 = tf.compat.v1.Variable(initial_b1, name='b1')\n",
    "\n",
    "#     layer1_in = tf.matmul(X, w1) + b1\n",
    "#     layer1_out = tf.nn.relu(layer1_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d1789",
   "metadata": {},
   "source": [
    "**Challenge:** Set up the second hidden layer. This layer has 64 neurons and needs to work off the outputs of the first hidden layer. Then setup the output layer. Remember, the output layer will use the softmax activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f129da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.compat.v1.name_scope('second_layer'):\n",
    "\n",
    "#     initial_w2 = tf.compat.v1.truncated_normal(shape=[n_hidden1, n_hidden2], stddev=0.1, seed=42)\n",
    "#     w2 = tf.compat.v1.Variable(initial_value=initial_w2, name='w2')\n",
    "\n",
    "#     initial_b2 = tf.constant(value=0.0, shape=[n_hidden2])\n",
    "#     b2 = tf.compat.v1.Variable(initial_b2, name='b2')\n",
    "\n",
    "#     layer2_in = tf.matmul(layer1_out, w2) + b2\n",
    "#     layer2_out = tf.nn.relu(layer2_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a5bc8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.compat.v1.name_scope('output_layer'):\n",
    "\n",
    "#     initial_w3 = tf.compat.v1.truncated_normal(shape=[n_hidden2, NR_CLASSES], stddev=0.1, seed=42)\n",
    "#     w3 = tf.compat.v1.Variable(initial_value=initial_w3, name='w3')\n",
    "\n",
    "#     initial_b3 = tf.constant(value=0.0, shape=[NR_CLASSES])\n",
    "#     b3 = tf.compat.v1.Variable(initial_b3, name='b3')\n",
    "\n",
    "#     layer3_in = tf.matmul(layer2_out, w3) + b3\n",
    "#     output = tf.nn.softmax(layer3_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cdb4b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(input, weight_dim, bias_dim, name):\n",
    "    \n",
    "    with tf.compat.v1.name_scope(name):\n",
    "        initial_w = tf.compat.v1.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n",
    "        w = tf.compat.v1.Variable(initial_value=initial_w, name='W')\n",
    "\n",
    "        initial_b = tf.constant(value=0.0, shape=bias_dim)\n",
    "        b = tf.compat.v1.Variable(initial_b, name='B')\n",
    "\n",
    "        layer_in = tf.matmul(input, w) + b\n",
    "        \n",
    "        if name=='out':\n",
    "            layer_out = tf.nn.softmax(layer_in)\n",
    "        else:\n",
    "            layer_out = tf.nn.relu(layer_in)\n",
    "            \n",
    "        tf.compat.v1.summary.histogram('weights', w)\n",
    "        tf.compat.v1.summary.histogram('biases', b)\n",
    "            \n",
    "        return layer_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1cd7ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model without Dropout\n",
    "# layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1], \n",
    "#                       bias_dim=[n_hidden1], name='layer_1')\n",
    "\n",
    "# layer_2 = setup_layer(layer_1, weight_dim=[n_hidden1, n_hidden2], \n",
    "#                       bias_dim=[n_hidden2], name='layer_2')\n",
    "\n",
    "# output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES], \n",
    "#                       bias_dim=[NR_CLASSES], name='out')\n",
    "\n",
    "# model_name = f'{n_hidden1}-{n_hidden2} LR{learning_rate} E {nr_epochs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c6d1a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with Dropout\n",
    "\n",
    "layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1], \n",
    "                      bias_dim=[n_hidden1], name='layer_1')\n",
    "\n",
    "layer_drop = tf.compat.v1.nn.dropout(layer_1, keep_prob=0.8, name='dropout_layer')\n",
    "\n",
    "layer_2 = setup_layer(layer_drop, weight_dim=[n_hidden1, n_hidden2], \n",
    "                      bias_dim=[n_hidden2], name='layer_2')\n",
    "\n",
    "output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES], \n",
    "                      bias_dim=[NR_CLASSES], name='out')\n",
    "\n",
    "model_name = f'{n_hidden1}-DO-{n_hidden2} LR{learning_rate} E {nr_epochs}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c94a22",
   "metadata": {},
   "source": [
    "## Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "73d40c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created directories!\n"
     ]
    }
   ],
   "source": [
    "# Folder for Tensorboard\n",
    "\n",
    "folder_name = f'{model_name} at {strftime(\"%H %M\")}'\n",
    "directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as exception:\n",
    "    print(exception.strerror)\n",
    "else:\n",
    "    print('Succesfully created directories!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6fcdea",
   "metadata": {},
   "source": [
    "## Loss, Optimization and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b9063",
   "metadata": {},
   "source": [
    "#### Defining Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e5ec06b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.name_scope('loss_calc'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3855792e",
   "metadata": {},
   "source": [
    "#### Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "880c0047",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.name_scope('optimizer'): \n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee6d8f",
   "metadata": {},
   "source": [
    "#### Defining the Accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "63159012",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.name_scope('accuracy_calc'):\n",
    "    model_prediction = tf.argmax(output, axis=1, name='prediction')\n",
    "    correct_pred = tf.equal(model_prediction, tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ecec61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0844b9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.name_scope('performance'):\n",
    "    tf.compat.v1.summary.scalar('accuracy', accuracy)\n",
    "    tf.compat.v1.summary.scalar('cost', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030951a",
   "metadata": {},
   "source": [
    "#### Check Input Images in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "496e809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.name_scope('show_image'):\n",
    "    x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "    tf.compat.v1.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ea6cb",
   "metadata": {},
   "source": [
    "## Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ba650321",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057f456",
   "metadata": {},
   "source": [
    "#### Setup Filewriter and Merge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ac22a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.compat.v1.summary.merge_all()\n",
    "# #merged_summary = tf.summary()\n",
    "\n",
    "train_writer = tf.compat.v1.summary.FileWriter(directory + '/train')\n",
    "train_writer.add_graph(sess.graph)\n",
    "\n",
    "validation_writer = tf.compat.v1.summary.FileWriter(directory + '/validation')\n",
    "validation_writer = tf.compat.v1.summary.FileWriter(directory + '/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baeb0ee",
   "metadata": {},
   "source": [
    "#### Initialize all the variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6adc3765",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5392b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w1.eval(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f7db5",
   "metadata": {},
   "source": [
    "### Batcing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8e88742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2dbaa773",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = y_train.shape[0]\n",
    "nr_iterations = int(num_examples/size_of_batch)\n",
    "\n",
    "index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d7c0fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    \n",
    "    global num_examples\n",
    "    global index_in_epoch\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > num_examples:\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        \n",
    "    end = index_in_epoch\n",
    "    \n",
    "    return data[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c00735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee201ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "afe59c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t| Training Accuracy = 0.8619999885559082\n",
      "Epoch 1 \t| Training Accuracy = 0.8809999823570251\n",
      "Epoch 2 \t| Training Accuracy = 0.9190000295639038\n",
      "Epoch 3 \t| Training Accuracy = 0.9629999995231628\n",
      "Epoch 4 \t| Training Accuracy = 0.9739999771118164\n",
      "Epoch 5 \t| Training Accuracy = 0.9750000238418579\n",
      "Epoch 6 \t| Training Accuracy = 0.9769999980926514\n",
      "Epoch 7 \t| Training Accuracy = 0.9789999723434448\n",
      "Epoch 8 \t| Training Accuracy = 0.9800000190734863\n",
      "Epoch 9 \t| Training Accuracy = 0.9810000061988831\n",
      "Epoch 10 \t| Training Accuracy = 0.9819999933242798\n",
      "Epoch 11 \t| Training Accuracy = 0.9850000143051147\n",
      "Epoch 12 \t| Training Accuracy = 0.9860000014305115\n",
      "Epoch 13 \t| Training Accuracy = 0.9850000143051147\n",
      "Epoch 14 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 15 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 16 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 17 \t| Training Accuracy = 0.9819999933242798\n",
      "Epoch 18 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 19 \t| Training Accuracy = 0.9900000095367432\n",
      "Done Training!!!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nr_epochs):\n",
    "    \n",
    "    ####========== Training Dataset ===========###########\n",
    "    for i in range(nr_iterations):\n",
    "        batch_x, batch_y = next_batch(batch_size=size_of_batch, \n",
    "                                      data=x_train, labels=y_train)\n",
    "        feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "        sess.run(train_step, feed_dict=feed_dictionary)\n",
    "        \n",
    "    s, batch_accuracy = sess.run(fetches=[merged_summary,accuracy], feed_dict=feed_dictionary)\n",
    "    train_writer.add_summary(s, epoch)\n",
    "        \n",
    "    print(f'Epoch {epoch} \\t| Training Accuracy = {batch_accuracy}')\n",
    "    \n",
    "    ####========== Validation Dataset ===========###########\n",
    "    summary = sess.run(fetches=merged_summary, feed_dict={X:x_val, Y:y_val})\n",
    "    validation_writer.add_summary(summary, epoch)\n",
    "\n",
    "print('Done Training!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e53ac500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(nr_epochs):\n",
    "#     for i in range(nr_iterations):\n",
    "#         batch_x, batch_y = next_batch(batch_size=size_of_batch, \n",
    "#                                       data=x_train, labels=y_train)\n",
    "#         feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "#         sess.run(train_step, feed_dict=feed_dictionary)\n",
    "        \n",
    "#     batch_accuracy = sess.run(fetches=[accuracy], feed_dict=feed_dictionary)\n",
    "        \n",
    "#     print(f'Epoch {epoch} \\t| Training Accuracy = {batch_accuracy}')\n",
    "        \n",
    "# print('Done Training!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e5187",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b50c8f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = {'accuracy_calc/prediction': model_prediction}\n",
    "# inputs = {'X': X}\n",
    "# tf.compat.v1.saved_model.simple_save(sess, 'SavedModel', inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a106e7f",
   "metadata": {},
   "source": [
    "## Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "67e753ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAYAAAByDd+UAAABAElEQVR4nO2Vyw2DMAyG3ap3RoAVmMCBrXICbwIbMEEEmzBCmMA9VFR9EGLTqhIV3xET/vjHjxMzM/yQ8y/FDsEgRAREtEnwJC2aWWAYhqfnzjmVoChDIroLISI45+5C2kwvsRemaYK+76HrOkiS5CmGiG8ZR2EB4zgGY0VRcF3Xks8wM7PI0jRNgzFEVCW4j7b4CLH5C3jv2RjD3nvxmWiVhphbJc/zt+pdY5NgWZYAcCuYqqp0hzUWWmvZGMPWWrX9KksfJ03TNKttEmNV8HWkqe1bIDi8iQjatoUsy8TNLbnQ6rbQDObZiej22Pz3F5DMVPE+/Bb/P0sPwf0LXgGAJwNqzP5nHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=28x28>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('MNIST/test_img.png')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c6e5a013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnElEQVR4nMVSwRGEIAxcHP+WcLZgBch15UtTynVgBcylE0vwKsg9HAhw6sNx5vYTwpIlGzCCY1Qn3G0kUZYabYjAAPxuJTGs90hr67D4vOcGgOVUVwKWLbgp7ojKPrZgr1kpITnWftWkzo8Sd82OFQDAE3Y8kB36Ib9Dycm5pWggyBIXikAcPL1adT8WZDpv1peRX8Tpmr9/sNvIL4ZNnNSjtr9VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert to grey scale using .convert() i.e to black and white\n",
    "bw = img.convert('L')\n",
    "bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "641d91ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array = np.invert(bw)\n",
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7e9f943a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Next we flatten it\n",
    "test_img = img_array.ravel()\n",
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6c6b0046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the test image is [2]\n"
     ]
    }
   ],
   "source": [
    "## In fetches argument, we are interested in highest number \n",
    "## in that row, hece we use the .argmax() funtion\n",
    "prediction = sess.run(fetches=tf.argmax(output, axis=1), feed_dict={X:[test_img]})\n",
    "print(f'Prediction for the test image is {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde79940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d9b0b86",
   "metadata": {},
   "source": [
    "## Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2dbdc1",
   "metadata": {},
   "source": [
    "**Challenge:** Calculate the accuracy over the test dataset (x_test and y_test). Use your knowledge of running a session to get the accuracy. Display as a percentage rounded to two decimal numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "22c1e0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 97.29%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = sess.run(fetches=accuracy, feed_dict={X:x_test, Y:y_test})\n",
    "print(f'Test Accuracy = {test_accuracy:0.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b2a32623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea00b460",
   "metadata": {},
   "source": [
    "## Reset for the Next Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "509327af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "sess.close()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d11317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abcbcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c617da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a4bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e7dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0452e594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae5ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
